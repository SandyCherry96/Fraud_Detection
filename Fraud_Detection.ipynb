{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa20d5e",
   "metadata": {},
   "source": [
    "# 💼 Problem Statement\n",
    "### Contributed By:Sandeep kumar \n",
    "With the growing adoption of digital payments, credit card fraud has emerged as a critical threat to both consumers and financial institutions. In FY’19 alone, over 52,000 cases of credit/debit card fraud were reported, reflecting the urgency of implementing proactive fraud detection mechanisms. Each fraudulent transaction results in a direct financial loss for the bank and undermines customer trust — making timely detection not just a security concern but a business imperative.\n",
    "\n",
    "This project aims to build an effective fraud detection system using machine learning techniques. By analyzing anonymized transaction data, addressing extreme class imbalance through SMOTE, and evaluating multiple classification models (Logistic Regression, Random Forest, and Gradient Boosting), we seek to accurately identify fraudulent transactions in real-time while minimizing false positives. The ultimate goal is to enhance financial security, protect consumers, and support banks in maintaining operational integrity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d03c5b9",
   "metadata": {},
   "source": [
    "## Step 1.Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f5866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f51359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "path = \"c:\\\\Users\\\\sanrkin\\\\Downloads\\\\creditcard.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Dataset Overview ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f24fd",
   "metadata": {},
   "source": [
    "PCA transformation is essential for protecting sensitive financial data while maintaining its analytical value for fraud detection.\n",
    "\n",
    "The V1-V28 features represent transformed versions of original transaction data, making it impossible for malicious actors to reverse-engineer sensitive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c59137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last few rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Data Info ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec5f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical info\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns in the dataset \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Fraud vs Normal Transactions\n",
    "df['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of fraud vs non-fraud transactions\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Target', data=df, palette=['#2ecc71', '#e74c3c'],  # Green for 0, Red for 1\n",
    "width=0.5)\n",
    "\n",
    "plt.title('Distribution of Target Classes (Fraud vs Normal)',fontsize=14, pad=15)\n",
    "plt.xlabel('Target Class', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "\n",
    "for i in plt.gca().containers:\n",
    "    plt.gca().bar_label(i, fmt='%d', padding=3)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.gca().set_axisbelow(True)\n",
    "\n",
    "plt.xticks([0, 1], ['Normal (0)', 'Fraud (1)'])\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='#2ecc71', label='Normal Transactions'),\n",
    "                  Patch(facecolor='#e74c3c', label='Fraudulent Transactions')]\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ab816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat map\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98a2ed",
   "metadata": {},
   "source": [
    "Low Multicollinearity: Most features (V1-V28) show low correlation with each other, indicated by the predominantly light-colored squares. This is good for model performance as it suggests that most features provide unique information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b5f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404af3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Target',axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed058f7",
   "metadata": {},
   "source": [
    "We used SMOTE to handle the extreme class imbalance in the dataset (~0.17% fraud). In such cases, traditional oversampling (which just duplicates existing fraud cases) can lead to overfitting, and undersampling (which removes normal cases) risks losing important information.\n",
    "\n",
    "Instead, SMOTE generates new, realistic synthetic fraud examples by learning patterns from existing ones. This helps the model better understand what fraud looks like without just memorizing data.\n",
    "\n",
    "It also improves something called recall, which means:\n",
    "\n",
    "\"Out of all real frauds, how many did we correctly catch?\"\n",
    "\n",
    "Improving recall is especially important in fraud detection because missing a fraud is much more costly than flagging a normal transaction by mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed878094",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot the distribution of fraud vs non-fraud transactions before and after SMOTE\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Before SMOTE\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=y_train, palette=['#2ecc71', '#e74c3c'])\n",
    "plt.title('Class Distribution Before SMOTE')\n",
    "plt.xlabel('Target Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Normal (0)', 'Fraud (1)'])\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "for i in plt.gca().containers:\n",
    "    plt.gca().bar_label(i)\n",
    "\n",
    "# Plot 2: After SMOTE\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=y_train_resampled, palette=['#2ecc71', '#e74c3c'])\n",
    "plt.title('Class Distribution After SMOTE')\n",
    "plt.xlabel('Target Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Normal (0)', 'Fraud (1)'])\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "for i in plt.gca().containers:\n",
    "    plt.gca().bar_label(i)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass distribution before SMOTE:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_resampled).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e43fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building and Training\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n===== {model_name} =====\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {'cm': cm, 'report': report, 'auc': auc}\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "    print(f\"ROC AUC Score: {auc:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Fraud'],\n",
    "                yticklabels=['Normal', 'Fraud'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for model_name, model in models.items():\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {results[model_name][\"auc\"]:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641363e8",
   "metadata": {},
   "source": [
    "### 🔍 1. Data Overview and Exploration\n",
    "Dataset size: 56,962 rows × 30 columns.\n",
    "\n",
    "Target Distribution:\n",
    "\n",
    "Normal (0): 56,864 transactions\n",
    "\n",
    "Fraud (1): 98 transactions (only ~0.17% → extremely imbalanced)\n",
    "\n",
    "\n",
    "\n",
    "### 🧪 PCA-Transformed Features:\n",
    "Features V1 to V28 are PCA components → real transaction features anonymized.\n",
    "\n",
    "These preserve pattern and variance but obscure sensitive data — a common step in financial datasets for privacy.\n",
    "\n",
    "\n",
    "\n",
    "### 🧼 Data Quality:\n",
    "✅ No missing values\n",
    "\n",
    "🔁 Low multicollinearity among features (confirmed via correlation heatmap)\n",
    "\n",
    "\n",
    "### 💡 Insight:\n",
    "The dataset is highly imbalanced and preprocessed using PCA, which helps with privacy but still needs techniques like SMOTE for better fraud detection.\n",
    "\n",
    "### ⚖️ 2. Class Imbalance Handling\n",
    "Problem: Very few fraud cases in training data → risk of models being biased toward \"normal\" transactions.\n",
    "\n",
    "Solution: Applied SMOTE (Synthetic Minority Oversampling Technique)\n",
    "\n",
    "Before SMOTE:\n",
    "Normal: 45,487 | Fraud: 82\n",
    "\n",
    "After SMOTE:\n",
    "Normal: 45,487 | Fraud: 45,487 (Balanced dataset)\n",
    "\n",
    "\n",
    "#### 📌 Insight:\n",
    "SMOTE creates synthetic fraud examples, increasing the model’s ability to identify frauds while maintaining the original data integrity — especially important for improving recall (catching actual frauds).\n",
    "\n",
    "\n",
    "### 🧠 3. Models Used\n",
    "You used three supervised learning models:\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Random Forest\n",
    "\n",
    "Gradient Boosting\n",
    "\n",
    "\n",
    "### 📈 4. Model Evaluation\n",
    "✅ Logistic Regression\n",
    "Recall (Fraud): 0.88\n",
    "\n",
    "Precision (Fraud): 0.06\n",
    "\n",
    "AUC: 0.9374\n",
    "\n",
    "⚠️ Warning: Model hit max iterations (you may want to increase max_iter)\n",
    "\n",
    "\n",
    "High recall but very low precision — flags too many false positives. This is common with logistic regression when classes are highly imbalanced or complex boundaries exist.\n",
    "\n",
    "✅ Random Forest\n",
    "Recall (Fraud): 0.81\n",
    "\n",
    "Precision (Fraud): 0.87\n",
    "\n",
    "AUC: 0.9296\n",
    "\n",
    "Accuracy: ~100%\n",
    "\n",
    "Insight:\n",
    "\n",
    "Well-balanced model — catches most frauds and minimizes false positives. Strong precision and recall. Good for production.\n",
    "\n",
    "✅ Gradient Boosting\n",
    "Recall (Fraud): 0.88\n",
    "\n",
    "Precision (Fraud): 0.29\n",
    "\n",
    "AUC: 0.9809 (highest)\n",
    "\n",
    "False Positives: 34 (a bit more than RF)\n",
    "\n",
    "Insight:\n",
    "\n",
    "Best AUC and recall, but lower precision than RF. Gradient Boosting is aggressive in identifying fraud but has a higher false positive rate.\n",
    "\n",
    "### 📊 5. ROC Curve Analysis\n",
    "Gradient Boosting clearly has the best ROC curve (AUC = 0.981), meaning it's best at separating fraud from non-fraud across all thresholds.\n",
    "\n",
    "Random Forest is also strong with a good balance.\n",
    "\n",
    "Logistic Regression performs adequately but suffers in precision.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
